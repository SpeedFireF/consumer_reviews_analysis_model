from nltk.translate.bleu_score import sentence_bleu
from nltk.tokenize import word_tokenize
from nltk.translate.meteor_score import single_meteor_score
from bert_score import score

from rouge import Rouge
rouge = Rouge()


def compute_metrics(text: list, tgt_text: list) -> dict:
    
    metrics = {}

    '''
    ROUGE is one of the most popular metrics for evaluating summarization quality. It measures the n-gram overlap between the generated summary and the reference summary. ROUGE is calculated for different n-grams (1-gram, 2-gram, 3-gram), and the scores are combined to obtain an overall score.
    '''
    rouge_score = rouge.get_scores(tgt_text, text)

    '''
    BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine’s output and that of a human: “the closer a machine translation is to a professional human translation, the better it is” 
    - this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.
    '''
    bleu_score = sentence_bleu(text, tgt_text)

    '''
    BERTScore is a text evaluation metric based on BERT language models. It measures the similarity between two sentences using the BERT representation of each word and calculating the F1 score, which is the harmonic mean between accuracy and coverage. BERTScore is capable of handling word, synonym, and antonym ambiguity issues, which makes it especially useful for assessing the quality of summaries generated by Transformer models.
    '''
    #pres, recall, f1
    bert_score = score(tgt_text, text, lang='en')

    '''
    METEOR is a metric that was originally developed to assess the quality of machine translations, but it can also be used to assess the quality of machine summaries. It evaluates the similarity between the generated summary and the reference summary, taking into account grammar and semantics.
    '''
    # batch_text = []
    # for sentence in text:
    #     sentence = word_tokenize(sentence)
    #     batch_text.append(sentence)
    
    # batch_tgt = []

    # for sentence in tgt_text:
    #     sentence = word_tokenize(sentence)
    #     batch_tgt.append(sentence)
    
    # meteor_Score = single_meteor_score(text[0], tgt_text[0])

    # print(rouge_score)
    # print(bleu_score)
    # print(bert_score)
    # print(meteor_Score)
    metrics['rough'] = rouge_score
    metrics['bleu'] = bleu_score
    metrics['bert_score'] = bert_score
    return metrics


if __name__ == '__main__':
    compute_metrics(['test1'], ['test2 fsf fd '])